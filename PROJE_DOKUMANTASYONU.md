# ğŸ“š Proje DokÃ¼mantasyonu - Semantic Search Engine

Bu dokÃ¼mantasyon, projedeki tÃ¼m dosyalarÄ±n ne iÅŸe yaradÄ±ÄŸÄ±nÄ±, hangi fonksiyonlarÄ±n ne yaptÄ±ÄŸÄ±nÄ± ve sistemin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± detaylÄ± olarak aÃ§Ä±klar.

---

## ğŸ“ Proje YapÄ±sÄ±

```
advanced_ir/
â”œâ”€â”€ app.py                  # FastAPI REST API sunucusu
â”œâ”€â”€ build_index.py          # Index oluÅŸturma scripti
â”œâ”€â”€ search_engine.py        # Ana arama motoru sÄ±nÄ±fÄ±
â”œâ”€â”€ streamlit_app.py        # Streamlit web arayÃ¼zÃ¼
â”œâ”€â”€ requirements.txt        # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ README.md              # Genel proje aÃ§Ä±klamasÄ±
â”œâ”€â”€ data/                  # Belgelerin bulunduÄŸu dizin
â”‚   â”œâ”€â”€ makine_ogrenmesi.txt
â”‚   â”œâ”€â”€ ornek_belge.txt
â”‚   â””â”€â”€ yapay_zeka.txt
â””â”€â”€ index/                 # OluÅŸturulan index dosyalarÄ±
    â”œâ”€â”€ faiss.index        # FAISS vektÃ¶r index'i
    â”œâ”€â”€ metadata.json      # Chunk metadata
    â””â”€â”€ doc_metadata.json  # Belge metadata
```

---

## ğŸ“– TEMEL KAVRAMLAR - DetaylÄ± AÃ§Ä±klama

### ğŸ”¸ CHUNK (ParÃ§a) Nedir?

**Basit AÃ§Ä±klama:** Chunk, bÃ¼yÃ¼k bir belgenin daha kÃ¼Ã§Ã¼k parÃ§alara bÃ¶lÃ¼nmÃ¼ÅŸ halidir. TÄ±pkÄ± bir kitabÄ± bÃ¶lÃ¼mlere ayÄ±rmak gibi!

**Neden Gerekli?**

1. **Model SÄ±nÄ±rlamalarÄ±:**
   - Transformer modelleri (BERT, vb.) genellikle maksimum 512 token (yaklaÅŸÄ±k 400-500 kelime) kabul eder
   - Uzun belgeleri tek seferde iÅŸleyemezler

2. **Hassas Arama:**
   - Belgenin sadece ilgili kÄ±smÄ±nÄ± bulmak iÃ§in
   - Ã–rnek: "Makine Ã¶ÄŸrenmesi nedir?" sorusuna 1000 sayfalÄ±k kitaptan sadece ilgili paragrafÄ± dÃ¶ndÃ¼rmek

3. **HÄ±zlÄ± Ä°ÅŸleme:**
   - KÃ¼Ã§Ã¼k parÃ§alar daha hÄ±zlÄ± iÅŸlenir
   - Gereksiz bilgileri filtreler

**GerÃ§ek Ã–rnek:**

Diyelim ki elimizde ÅŸu belge var (makine_ogrenmesi.txt):

```
Makine Ã–ÄŸrenmesine GiriÅŸ

1. Makine Ã–ÄŸrenmesi Nedir?
Makine Ã¶ÄŸrenmesi (ML), bilgisayarlarÄ±n aÃ§Ä±kÃ§a programlanmadan veriden Ã¶ÄŸrenmesini saÄŸlayan algoritmalar bÃ¼tÃ¼nÃ¼dÃ¼r...

2. Makine Ã–ÄŸrenmesi TÃ¼rleri
Makine Ã¶ÄŸrenmesi genel olarak Ã¼Ã§ ana kategoriye ayrÄ±lÄ±r...

3. GÃ¶zetimli Ã–ÄŸrenme
Bu yÃ¶ntemde algoritma, hem girdileri hem de Ã§Ä±ktÄ±larÄ± iÃ§eren etiketli verilerle eÄŸitilir...
```

**200 kelimelik chunk'lara bÃ¶lÃ¼ndÃ¼ÄŸÃ¼nde:**

```
CHUNK 1 (doc_id: 0, chunk_id: 0):
"Makine Ã–ÄŸrenmesine GiriÅŸ 1. Makine Ã–ÄŸrenmesi Nedir? Makine Ã¶ÄŸrenmesi (ML)... [200 kelime]"

CHUNK 2 (doc_id: 0, chunk_id: 1):
"... [kalan kelimeler, 200 kelime]"

CHUNK 3 (doc_id: 0, chunk_id: 2):
"... [devamÄ±, 200 kelime]"
```

**Metadata'da NasÄ±l GÃ¶rÃ¼nÃ¼r?**

```json
{
    "doc_id": 0,                          // Hangi belge?
    "chunk_id": 0,                        // Belgenin kaÃ§Ä±ncÄ± parÃ§asÄ±?
    "text": "Makine Ã¶ÄŸrenmesi (ML)...",   // ParÃ§anÄ±n iÃ§eriÄŸi
    "doc_name": "makine_ogrenmesi.txt",   // Belge adÄ±
    "doc_hash": "eea0f046..."             // Belgenin benzersiz kodu
}
```

**Chunk Boyutu Neden 200 Kelime?**

- **Ã‡ok kÃ¼Ã§Ã¼k (50-100 kelime):** Ã‡ok fazla parÃ§a oluÅŸur, arama yavaÅŸlar
- **Orta (200 kelime):** Ä°yi dengeli, yeterli context + hÄ±zlÄ± arama
- **BÃ¼yÃ¼k (500+ kelime):** Model sÄ±nÄ±rlarÄ±nÄ± zorlar, daha az hassas

---

### ğŸ”¸ FAISS INDEX Nedir ve Ä°Ã§inde Ne Var?

**Basit AÃ§Ä±klama:** FAISS index, metinlerin matematiksel gÃ¶sterimlerini (vektÃ¶rler) saklayan ve hÄ±zlÄ± arama yapmamÄ±zÄ± saÄŸlayan bir veritabanÄ±dÄ±r.

**Analoji:** 
- Normal arama = Kitapta kelime kelime aramak (yavaÅŸ)
- FAISS index = Her sayfanÄ±n Ã¶zetini numaralÄ± kartlarda saklamak, kart numarasÄ±na gÃ¶re hÄ±zlÄ± bulmak (Ã§ok hÄ±zlÄ±)

**FAISS Index Ä°Ã§eriÄŸi - DetaylÄ± AÃ§Ä±klama:**

FAISS index dosyasÄ± (`index/faiss.index`) bir **binary (ikili) dosyadÄ±r**. Ä°nsanlar tarafÄ±ndan doÄŸrudan okunamaz, sadece FAISS kÃ¼tÃ¼phanesi ile okunabilir.

**1. Dosya FormatÄ±:**
- **Tip:** Binary (ikili)
- **AÃ§Ä±klama:** Normal metin dosyasÄ± deÄŸil, Ã¶zel format
- **Okuma:** Sadece `faiss.read_index()` ile okunabilir
- **DÃ¼zenleme:** DoÄŸrudan dÃ¼zenlenemez, yeniden oluÅŸturulmalÄ±

**2. Ä°Ã§eriÄŸi - Ne SaklanÄ±yor?**

FAISS index dosyasÄ± iÃ§inde ÅŸunlar saklanÄ±r:

**A) VektÃ¶r Verileri:**
- Her chunk'Ä±n 384 boyutlu sayÄ±sal gÃ¶sterimi
- Ã–rnek vektÃ¶r: `[0.234, -0.567, 0.891, 0.123, ..., -0.456]` (384 sayÄ±)
- Her sayÄ± `float32` formatÄ±nda (4 byte)

**B) Index BaÅŸlÄ±k Bilgileri:**
- Index tipi: `IndexFlatL2`
- VektÃ¶r boyutu: `384`
- Toplam vektÃ¶r sayÄ±sÄ±: `125` (Ã¶rnek)

**C) VektÃ¶r Organizasyonu:**
- VektÃ¶rler sÄ±ralÄ± ÅŸekilde saklanÄ±r
- Her vektÃ¶rÃ¼n pozisyonu (index numarasÄ±) kaydedilir
- HÄ±zlÄ± eriÅŸim iÃ§in optimize edilmiÅŸ yapÄ±

**3. Dosya Boyutu Hesaplama:**

```
Toplam Boyut = VektÃ¶r SayÄ±sÄ± Ã— VektÃ¶r Boyutu Ã— Byte Per SayÄ±
             = 125 Ã— 384 Ã— 4 byte
             = 192.000 byte
             â‰ˆ 188 KB
```

**4. Ä°Ã§erik Ã–rneÄŸi (GÃ¶rsel Temsil):**

```
FAISS INDEX DOSYASI (faiss.index)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[BAÅLIK BÄ°LGÄ°LERÄ°]
  Index Tipi: IndexFlatL2
  VektÃ¶r Boyutu: 384
  Toplam VektÃ¶r: 125
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[VEKTÃ–R 0] â†’ [0.234, -0.567, 0.891, ..., 0.123] (384 sayÄ±)
[VEKTÃ–R 1] â†’ [0.245, -0.578, 0.902, ..., 0.134] (384 sayÄ±)
[VEKTÃ–R 2] â†’ [0.256, -0.589, 0.913, ..., 0.145] (384 sayÄ±)
...
[VEKTÃ–R 124] â†’ [0.890, -0.123, 0.456, ..., 0.789] (384 sayÄ±)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**5. FAISS Index ile Metadata.json Ä°liÅŸkisi:**

```
FAISS Index (faiss.index)        Metadata (metadata.json)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•         â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VektÃ¶r 0 (sadece sayÄ±lar)    â†’   Chunk 0 (metin + bilgiler)
  [0.234, -0.567, ...]       â†’   {
                                    "text": "Makine Ã¶ÄŸrenmesi...",
                                    "doc_id": 0,
                                    "doc_name": "makine_ogrenmesi.txt"
                                  }

VektÃ¶r 1 (sadece sayÄ±lar)    â†’   Chunk 1 (metin + bilgiler)
  [0.245, -0.578, ...]       â†’   {
                                    "text": "GÃ¶zetimli Ã¶ÄŸrenme...",
                                    "doc_id": 0,
                                    "doc_name": "makine_ogrenmesi.txt"
                                  }
```

**NasÄ±l Birlikte KullanÄ±lÄ±rlar?**

1. FAISS index sadece **sayÄ±sal arama** iÃ§in kullanÄ±lÄ±r (hÄ±zlÄ±)
2. Metadata.json **metin iÃ§eriÄŸi** iÃ§in kullanÄ±lÄ±r (sonuÃ§larÄ± gÃ¶stermek iÃ§in)
3. Arama sonucunda:
   - FAISS â†’ Index numarasÄ±nÄ± verir (Ã¶rnek: 5)
   - Metadata â†’ Index 5'teki metni verir

**6. GÃ¶rsel AÃ§Ä±klama - AkÄ±ÅŸ:**

```
ORJÄ°NAL METÄ°N                    â†’    VEKÃ–R (384 sayÄ±)

"Makine Ã¶ÄŸrenmesi nedir?"        â†’    [0.23, -0.56, 0.89, ..., 0.12]
                                      â†“
                                  FAISS INDEX (faiss.index)
                                      â†“ (Arama: En yakÄ±n 5 vektÃ¶r)
                            HÄ±zlÄ± benzerlik aramasÄ±
                                      â†“
                           Index numaralarÄ±: [2, 0, 5, 8, 12]
                                      â†“
                                  Metadata.json
                                      â†“ (Index 2'deki metni al)
                           "Makine Ã¶ÄŸrenmesi (ML)..."
                                      â†“
                                  KullanÄ±cÄ±ya gÃ¶ster
```

**7. FAISS Index TÃ¼rleri:**

**IndexFlatL2 (Åu an kullanÄ±lan):**
- âœ… Kesin sonuÃ§ verir
- âœ… Basit ve anlaÅŸÄ±lÄ±r
- âŒ BÃ¼yÃ¼k veri setlerinde yavaÅŸ (10.000+ vektÃ¶r)

**Alternatifler:**

**IndexIVFFlat:**
- âœ… Daha hÄ±zlÄ± (bÃ¼yÃ¼k veri setleri iÃ§in)
- âŒ YaklaÅŸÄ±k sonuÃ§lar (biraz hata payÄ±)

**IndexHNSW:**
- âœ… Ã‡ok hÄ±zlÄ±
- âœ… Hassas sonuÃ§lar
- âŒ Daha fazla bellek kullanÄ±r

**8. FAISS Index Okuma (Python ile):**

```python
import faiss

# Index'i yÃ¼kle
index = faiss.read_index("index/faiss.index")

# Bilgileri gÃ¶r
print(f"Toplam vektÃ¶r: {index.ntotal}")      # 125
print(f"VektÃ¶r boyutu: {index.d}")           # 384

# Belirli bir vektÃ¶rÃ¼ oku
vektÃ¶r_5 = index.reconstruct(5)  # 5. vektÃ¶rÃ¼ getir
print(f"VektÃ¶r 5: {vektÃ¶r_5[:10]}...")  # Ä°lk 10 sayÄ±sÄ±nÄ± gÃ¶ster
```

**9. FAISS Index AvantajlarÄ±:**

âœ… **HÄ±z:** Milisaniyeler iÃ§inde milyonlarca vektÃ¶r arasÄ±nda arama
âœ… **Bellek:** Verimli bellek kullanÄ±mÄ±
âœ… **Ã–lÃ§eklenebilirlik:** BÃ¼yÃ¼k veri setlerini destekler
âœ… **DoÄŸruluk:** Matematiksel olarak kesin sonuÃ§lar

**10. Ã–nemli Notlar:**

- âŒ FAISS index dosyasÄ±nÄ± doÄŸrudan dÃ¼zenleyemezsiniz
- âŒ Yeni belge eklediÄŸinizde index'i yeniden oluÅŸturmalÄ±sÄ±nÄ±z
- âœ… Metadata.json dosyasÄ±nÄ± okuyabilirsiniz (normal metin dosyasÄ±)
- âœ… Index'i silip yeniden oluÅŸturabilirsiniz

**FAISS Index DosyasÄ±nÄ±n Ä°Ã§eriÄŸi:**

FAISS index dosyasÄ± binary (ikili) formattadÄ±r, bu yÃ¼zden doÄŸrudan okuyamazsÄ±nÄ±z. Ancak iÃ§inde ÅŸunlar saklanÄ±r:

```
FAISS Index Ä°Ã§eriÄŸi:
â”œâ”€â”€ Index Tipi: IndexFlatL2
â”œâ”€â”€ VektÃ¶r Boyutu: 384
â”œâ”€â”€ Toplam VektÃ¶r SayÄ±sÄ±: 125 (Ã¶rnek)
â””â”€â”€ Her VektÃ¶r:
    â”œâ”€â”€ 384 adet float32 sayÄ±sÄ±
    â”œâ”€â”€ Ã–rnek: [0.234, -0.567, ..., 0.123]
    â””â”€â”€ Toplam: 384 Ã— 4 byte = 1.536 byte per vektÃ¶r
```

**Ã–rnek Hesaplama:**
- 125 chunk var
- Her chunk = 384 boyutlu vektÃ¶r
- Her sayÄ± = 4 byte (float32)
- **Toplam boyut:** 125 Ã— 384 Ã— 4 = **192.000 byte â‰ˆ 188 KB**

**Neden FAISS KullanÄ±yoruz?**

1. **HÄ±z:**
   - Normal arama: TÃ¼m metinleri karÅŸÄ±laÅŸtÄ±r (Ã§ok yavaÅŸ)
   - FAISS: Matematiksel mesafe hesaplamasÄ± (Ã§ok hÄ±zlÄ±)
   - 10.000 chunk'ta bile milisaniyeler iÃ§inde sonuÃ§

2. **Ã–lÃ§eklenebilirlik:**
   - Milyonlarca vektÃ¶rÃ¼ saklayabilir
   - Bellek kullanÄ±mÄ±nÄ± optimize eder

3. **DoÄŸruluk:**
   - Semantik (anlamsal) benzerliÄŸi yakalar
   - "Makine Ã¶ÄŸrenmesi" = "ML" = "machine learning" (aynÄ± anlam)

---

### ğŸ”¸ VektÃ¶r (Embedding) Nedir?

**Basit AÃ§Ä±klama:** VektÃ¶r, bir metnin matematiksel gÃ¶sterimidir. SayÄ±sal bir dizi ile metnin anlamÄ±nÄ± temsil ederiz.

**GerÃ§ek Ã–rnek:**

Metin: `"Makine Ã¶ÄŸrenmesi, veriden Ã¶ÄŸrenen algoritmalardÄ±r."`

Bu metin SentenceTransformer modeli tarafÄ±ndan ÅŸu ÅŸekilde vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r:

```python
# 384 boyutlu vektÃ¶r (ilk 10 boyutu gÃ¶steriliyor)
[ 0.234, -0.567,  0.891, -0.123,  0.456,
 -0.789,  0.321, -0.654,  0.987, -0.234,
 ... 374 tane daha sayÄ± ...]
```

**Bu SayÄ±lar Ne Anlama Geliyor?**

- Her sayÄ±, metnin belirli bir Ã¶zelliÄŸini temsil eder
- Model, eÄŸitim sÄ±rasÄ±nda hangi sayÄ±larÄ±n ne anlama geldiÄŸini Ã¶ÄŸrenir
- Benzer anlamlÄ± metinler, benzer sayÄ±sal deÄŸerlere sahip olur

**KarÅŸÄ±laÅŸtÄ±rma Ã–rneÄŸi:**

```python
# Metin 1: "Makine Ã¶ÄŸrenmesi nedir?"
vektÃ¶r1 = [0.23, -0.56, 0.89, ...]

# Metin 2: "Machine learning ne demek?" (Ä°ngilizce ama aynÄ± anlam)
vektÃ¶r2 = [0.24, -0.55, 0.88, ...]

# Mesafe hesaplama (L2):
mesafe = sqrt((0.23-0.24)Â² + (-0.56-(-0.55))Â² + ...)
# KÃ¼Ã§Ã¼k mesafe = Benzer anlam!
```

**Neden 384 Boyut?**

- Model: `all-MiniLM-L6-v2`
- 384 boyut = Ä°yi dengeli (hÄ±z + kalite)
- Daha az boyut (128): Daha hÄ±zlÄ± ama daha az hassas
- Daha Ã§ok boyut (768): Daha hassas ama daha yavaÅŸ

---

### ğŸ”¸ Index DosyalarÄ±nÄ±n Birlikte Ã‡alÄ±ÅŸmasÄ±

Projede 3 Ã¶nemli index dosyasÄ± var:

#### 1. `faiss.index` (Binary Dosya)
**Ä°Ã§erik:** Sadece sayÄ±lar (vektÃ¶rler)
**Okuma:** FAISS kÃ¼tÃ¼phanesi ile okunur
**Boyut:** ~188 KB (125 chunk iÃ§in)

#### 2. `metadata.json` (Metin DosyasÄ±)
**Ä°Ã§erik:** Her chunk'Ä±n metin iÃ§eriÄŸi ve bilgileri
**Ã–rnek:**
```json
[
    {
        "doc_id": 0,
        "chunk_id": 0,
        "text": "Makine Ã–ÄŸrenmesine GiriÅŸ...",
        "doc_name": "makine_ogrenmesi.txt",
        "doc_hash": "eea0f046..."
    },
    {
        "doc_id": 0,
        "chunk_id": 1,
        "text": "regresyon KÃ¼meleme...",
        "doc_name": "makine_ogrenmesi.txt",
        "doc_hash": "eea0f046..."
    }
]
```

#### 3. `doc_metadata.json` (Metin DosyasÄ±)
**Ä°Ã§erik:** Belgelerin genel bilgileri
**Ã–rnek:**
```json
[
    {
        "doc_id": 0,
        "name": "makine_ogrenmesi.txt",
        "hash": "eea0f046...",
        "chunk_count": 2,
        "created_at": "2025-12-03T14:31:36"
    }
]
```

**NasÄ±l Birlikte Ã‡alÄ±ÅŸÄ±rlar?**

```
1. KullanÄ±cÄ± sorgu girer: "Makine Ã¶ÄŸrenmesi nedir?"

2. FAISS Index'te arama:
   - Sorgu vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
   - FAISS en yakÄ±n 5 vektÃ¶rÃ¼ bulur (index: 0, 15, 23, 45, 67)

3. Metadata'dan metinleri al:
   - Index 0 â†’ metadata.json[0]["text"] = "Makine Ã¶ÄŸrenmesi (ML)..."
   - Index 15 â†’ metadata.json[15]["text"] = "..."

4. Belge bilgilerini al:
   - metadata.json[0]["doc_id"] = 0
   - doc_metadata.json[0]["name"] = "makine_ogrenmesi.txt"

5. SonuÃ§ gÃ¶ster:
   - "makine_ogrenmesi.txt" belgesinden
   - "Makine Ã¶ÄŸrenmesi (ML)..." parÃ§asÄ±
   - Benzerlik skoru: 0.85
```

---

## 1ï¸âƒ£ search_engine.py - Ana Arama Motoru

**DosyanÄ±n AmacÄ±:** Projenin kalbi olan bu dosya, tÃ¼m arama, indeksleme ve belge iÅŸleme fonksiyonlarÄ±nÄ± iÃ§erir. `SearchEngine` sÄ±nÄ±fÄ±, FAISS vektÃ¶r veritabanÄ± ve sentence-transformers kullanarak semantik arama yapar.

### ğŸ“¦ Ä°Ã§e AktarÄ±lan KÃ¼tÃ¼phaneler

```python
import faiss                    # Facebook'un vektÃ¶r benzerliÄŸi arama kÃ¼tÃ¼phanesi
import numpy as np              # SayÄ±sal iÅŸlemler iÃ§in
import json                     # JSON dosya iÅŸlemleri
import os                       # Dosya sistemi iÅŸlemleri
from sentence_transformers import SentenceTransformer  # Metin â†’ VektÃ¶r dÃ¶nÃ¼ÅŸÃ¼mÃ¼
import logging                  # Log kayÄ±tlarÄ±
from transformers import pipeline  # BERT modeli iÃ§in soru-cevap
from datetime import datetime    # Tarih iÅŸlemleri
import hashlib                  # MD5 hash hesaplama
import time                     # Zaman Ã¶lÃ§Ã¼mÃ¼
```

### ğŸ”§ SearchEngine SÄ±nÄ±fÄ±

#### `__init__(self, index_path, metadata_path, doc_metadata_path)`
**SatÄ±rlar:** 30-44

**Ne Yapar:**
- SearchEngine nesnesini baÅŸlatÄ±r
- Gerekli dosya yollarÄ±nÄ± ayarlar
- Sentence Transformer modelini yÃ¼kler (`all-MiniLM-L6-v2`)
- Index ve belgeleri tutacak listeleri hazÄ±rlar
- Gerekli dizinleri oluÅŸturur

**Parametreler:**
- `index_path`: FAISS index dosyasÄ±nÄ±n yolu (varsayÄ±lan: "index/faiss.index")
- `metadata_path`: Metadata JSON dosyasÄ±nÄ±n yolu (varsayÄ±lan: "index/metadata.json")
- `doc_metadata_path`: Belge metadata JSON dosyasÄ±nÄ±n yolu (varsayÄ±lan: "index/doc_metadata.json")

**Ä°Ã§ DeÄŸiÅŸkenler:**
- `self.model`: Sentence Transformer modeli (384 boyutlu vektÃ¶rler Ã¼retir)
- `self.index`: FAISS index nesnesi (None baÅŸlangÄ±Ã§ta)
- `self.docs`: TÃ¼m chunk'larÄ±n metadata listesi
- `self.doc_metadata`: Belgelerin genel bilgileri
- `self.summarizer`: Ã–zetleme modeli (henÃ¼z kullanÄ±lmÄ±yor)
- `self.qa_pipeline`: Soru-cevap modeli (lazy loading ile yÃ¼klenir)

---

#### `_ensure_directories(self)`
**SatÄ±rlar:** 46-51

**Ne Yapar:**
- Index dosyalarÄ±nÄ±n kaydedileceÄŸi dizini oluÅŸturur
- EÄŸer `index/` dizini yoksa oluÅŸturur

**KullanÄ±m:** Otomatik olarak `__init__` iÃ§inde Ã§aÄŸrÄ±lÄ±r.

---

#### `_save_doc_metadata(self, doc_metadata)`
**SatÄ±rlar:** 53-57

**Ne Yapar:**
- Belge metadata'sÄ±nÄ± JSON dosyasÄ±na kaydeder
- UTF-8 encoding kullanÄ±r, TÃ¼rkÃ§e karakterleri destekler
- Ä°ndentli (4 boÅŸluk) JSON formatÄ±nda kaydeder

**Parametreler:**
- `doc_metadata`: Belge bilgilerini iÃ§eren liste

**Ã–rnek Metadata YapÄ±sÄ±:**
```json
{
    "doc_id": 0,
    "name": "makine_ogrenmesi.txt",
    "hash": "eea0f04690dda0320fed866cfe1335f6",
    "chunk_count": 15,
    "created_at": "2024-01-15T10:30:00"
}
```

---

#### `_load_doc_metadata(self)`
**SatÄ±rlar:** 59-68

**Ne Yapar:**
- KaydedilmiÅŸ belge metadata'sÄ±nÄ± yÃ¼kler
- EÄŸer dosya yoksa uyarÄ± verir ve False dÃ¶ndÃ¼rÃ¼r

**DÃ¶nÃ¼ÅŸ DeÄŸeri:**
- `True`: BaÅŸarÄ±lÄ± yÃ¼kleme
- `False`: Dosya bulunamadÄ±

---

#### `chunk_text(self, text, chunk_size=200)`
**SatÄ±rlar:** 71-77

**Ne Yapar:**
- Uzun metinleri daha kÃ¼Ã§Ã¼k parÃ§alara (chunk) bÃ¶ler
- Her parÃ§a 200 kelimelik olur (varsayÄ±lan)
- Basit kelime bazlÄ± bÃ¶lme yapar (cÃ¼mle sÄ±nÄ±rlarÄ±nÄ± dikkate almaz)

**Parametreler:**
- `text`: BÃ¶lÃ¼necek metin (string)
- `chunk_size`: Her chunk'taki kelime sayÄ±sÄ± (varsayÄ±lan: 200)

**DetaylÄ± AÃ§Ä±klama:**

**AdÄ±m AdÄ±m NasÄ±l Ã‡alÄ±ÅŸÄ±r:**

1. **Metni Kelimelere AyÄ±r:**
   ```python
   words = text.split()  # BoÅŸluklardan ayÄ±rÄ±r
   ```
   - Ã–rnek: `"Makine Ã¶ÄŸrenmesi nedir? Ã‡ok Ã¶nemli bir konu."`
   - SonuÃ§: `["Makine", "Ã¶ÄŸrenmesi", "nedir?", "Ã‡ok", "Ã¶nemli", "bir", "konu."]`

2. **200'ÅŸer Kelimelik Gruplar OluÅŸtur:**
   ```python
   for i in range(0, len(words), chunk_size):
       chunk = " ".join(words[i:i + chunk_size])
   ```
   - Ä°lk 200 kelime â†’ Chunk 0
   - Sonraki 200 kelime â†’ Chunk 1
   - Devam eder...

3. **Chunk'larÄ± Listeye Ekle:**
   - Her chunk bir string olarak listeye eklenir

**GerÃ§ek Ã–rnek:**

Diyelim ki elimizde 550 kelimelik bir metin var:

```python
text = """
Makine Ã¶ÄŸrenmesi (ML), bilgisayarlarÄ±n aÃ§Ä±kÃ§a programlanmadan 
veriden Ã¶ÄŸrenmesini saÄŸlayan algoritmalar bÃ¼tÃ¼nÃ¼dÃ¼r. 
AmaÃ§, geÃ§miÅŸ verilere bakarak yeni Ã¶rnekler Ã¼zerinde tahmin 
veya karar verebilen modeller geliÅŸtirmektir. Makine Ã¶ÄŸrenmesi 
genel olarak Ã¼Ã§ ana kategoriye ayrÄ±lÄ±r: GÃ¶zetimli Ã¶ÄŸrenme, 
gÃ¶zetimsiz Ã¶ÄŸrenme ve pekiÅŸtirmeli Ã¶ÄŸrenme. GÃ¶zetimli Ã¶ÄŸrenmede 
algoritma, hem girdileri hem de Ã§Ä±ktÄ±larÄ± iÃ§eren etiketli 
verilerle eÄŸitilir. GÃ¶zetimsiz Ã¶ÄŸrenmede ise veriler etiketli 
deÄŸildir ve algoritma verideki yapÄ±larÄ± keÅŸfetmeye Ã§alÄ±ÅŸÄ±r. 
PekiÅŸtirmeli Ã¶ÄŸrenmede model, bir ortam iÃ§inde kararlar alÄ±r 
ve her aksiyon sonrasÄ± Ã¶dÃ¼l veya ceza alÄ±r. [550 kelime toplam]
"""

chunks = engine.chunk_text(text, chunk_size=200)

# SonuÃ§:
# chunks[0] = Ä°lk 200 kelime (200 kelime)
# chunks[1] = Sonraki 200 kelime (200 kelime)  
# chunks[2] = Kalan 150 kelime (150 kelime)
# Toplam: 3 chunk
```

**Neden CÃ¼mle SÄ±nÄ±rlarÄ±nÄ± Dikkate AlmÄ±yor?**

- **Basitlik:** Daha hÄ±zlÄ± ve anlaÅŸÄ±lÄ±r kod
- **Yeterlilik:** 200 kelime genellikle birkaÃ§ cÃ¼mle iÃ§erir
- **HÄ±z:** CÃ¼mle analizi daha yavaÅŸ olur

**GeliÅŸtirme Ã–nerisi:**
Daha iyi chunk'lar iÃ§in cÃ¼mle sÄ±nÄ±rlarÄ±nÄ± dikkate alabilirsiniz:
```python
# Ã–rnek geliÅŸtirme (ÅŸu an kullanÄ±lmÄ±yor)
def chunk_text_smart(text, chunk_size=200):
    sentences = text.split('.')
    chunks = []
    current_chunk = []
    current_words = 0
    
    for sentence in sentences:
        words = sentence.split()
        if current_words + len(words) <= chunk_size:
            current_chunk.append(sentence)
            current_words += len(words)
        else:
            chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_words = len(words)
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    return chunks
```

**Neden Ã–nemli:**
- FAISS ve transformer modelleri uzun metinleri iÅŸlemekte zorlanÄ±r
- Chunk'lar ayrÄ± ayrÄ± indekslenir, bÃ¶ylece daha hassas arama yapÄ±lÄ±r
- Sadece ilgili kÄ±sÄ±m dÃ¶ndÃ¼rÃ¼lÃ¼r, tÃ¼m belge deÄŸil

---

#### `load_pdf(self, path)`
**SatÄ±rlar:** 80-91

**Ne Yapar:**
- PDF dosyasÄ±ndan metni Ã§Ä±karÄ±r
- PyPDF2 kÃ¼tÃ¼phanesini kullanÄ±r
- TÃ¼m sayfalarÄ± birleÅŸtirir
- Hata durumunda uyarÄ± verir ama devam eder

**Parametreler:**
- `path`: PDF dosyasÄ±nÄ±n yolu

**DÃ¶nÃ¼ÅŸ DeÄŸeri:**
- Ã‡Ä±karÄ±lan metin (string)

**Hata YÃ¶netimi:**
- EÄŸer bir sayfa okunamazsa, o sayfayÄ± atlar ve devam eder
- Logger ile uyarÄ± kaydeder

---

#### `build_index(self, documents, doc_names=None)`
**SatÄ±rlar:** 94-138

**Ne Yapar:**
- Belgeleri chunk'lara bÃ¶ler
- Her chunk'Ä± vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (embedding)
- FAISS index'i oluÅŸturur
- Metadata'larÄ± JSON dosyalarÄ±na kaydeder

**Parametreler:**
- `documents`: Metin listesi (her eleman bir belge)
  - Ã–rnek: `["Makine Ã¶ÄŸrenmesi metni...", "Yapay zeka metni..."]`
- `doc_names`: Belge isimleri listesi (opsiyonel)
  - Ã–rnek: `["makine_ogrenmesi.txt", "yapay_zeka.txt"]`
  - Verilmezse: `["Belge_1", "Belge_2", ...]` otomatik oluÅŸturulur

**DetaylÄ± Ä°ÅŸlem AdÄ±mlarÄ±:**

**1. ADIM: Belgeleri Chunk'lara BÃ¶lme**

```python
for doc_id, doc in enumerate(documents):
    chunks = self.chunk_text(doc)  # 200 kelimelik parÃ§alara bÃ¶ler
```

**Ã–rnek:**
- Belge 0: 550 kelime â†’ 3 chunk (200, 200, 150 kelime)
- Belge 1: 800 kelime â†’ 4 chunk (200, 200, 200, 200 kelime)
- **Toplam:** 7 chunk

**2. ADIM: Hash (Benzersiz Kod) Hesaplama**

```python
doc_hash = hashlib.md5(doc.encode('utf-8')).hexdigest()
```

**Ne Ä°ÅŸe Yarar?**
- AynÄ± belgenin tekrar yÃ¼klenip yÃ¼klenmediÄŸini kontrol eder
- Ã–rnek hash: `"eea0f04690dda0320fed866cfe1335f6"` (32 karakter)

**3. ADIM: Belge Metadata OluÅŸturma**

Her belge iÃ§in ÅŸu bilgiler kaydedilir:

```python
doc_info = {
    "doc_id": 0,                              # Belge numarasÄ± (0'dan baÅŸlar)
    "name": "makine_ogrenmesi.txt",          # Dosya adÄ±
    "hash": "eea0f046...",                   # MD5 hash
    "chunk_count": 3,                        # KaÃ§ parÃ§aya bÃ¶lÃ¼ndÃ¼?
    "created_at": "2025-12-03T14:31:36"     # OluÅŸturulma tarihi
}
```

**4. ADIM: Chunk Metadata OluÅŸturma**

Her chunk iÃ§in ÅŸu bilgiler kaydedilir:

```python
metadata.append({
    "doc_id": 0,                             # Hangi belgeye ait?
    "chunk_id": 0,                           # Belgenin kaÃ§Ä±ncÄ± parÃ§asÄ±?
    "text": "Makine Ã¶ÄŸrenmesi (ML)...",     # ParÃ§anÄ±n iÃ§eriÄŸi
    "doc_name": "makine_ogrenmesi.txt",     # Belge adÄ±
    "doc_hash": "eea0f046..."               # Belgenin hash'i
})
```

**Ã–rnek Metadata.json:**
```json
[
    {
        "doc_id": 0,
        "chunk_id": 0,
        "text": "Makine Ã¶ÄŸrenmesi (ML)... [200 kelime]",
        "doc_name": "makine_ogrenmesi.txt",
        "doc_hash": "eea0f046..."
    },
    {
        "doc_id": 0,
        "chunk_id": 1,
        "text": "... [kalan 200 kelime]",
        "doc_name": "makine_ogrenmesi.txt",
        "doc_hash": "eea0f046..."
    }
]
```

**5. ADIM: VektÃ¶rleÅŸtirme (Embedding)**

```python
embeddings = self.model.encode(all_chunks).astype("float32")
```

**Ne Oluyor?**
- TÃ¼m chunk'lar bir seferde vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r (batch processing)
- Her chunk â†’ 384 boyutlu vektÃ¶r
- 7 chunk varsa â†’ 7 Ã— 384 = 2.688 sayÄ±

**Ã–rnek:**
```
Chunk 0: "Makine Ã¶ÄŸrenmesi..." 
  â†’ [0.234, -0.567, 0.891, ..., 0.123] (384 sayÄ±)

Chunk 1: "GÃ¶zetimli Ã¶ÄŸrenme..."
  â†’ [0.245, -0.578, 0.902, ..., 0.134] (384 sayÄ±)

... (tÃ¼m chunk'lar)
```

**Neden Float32?**
- FAISS float32 formatÄ± bekler
- Daha az bellek kullanÄ±r (float64 yerine)
- Yeterince hassas

**6. ADIM: FAISS Index OluÅŸturma**

```python
dim = embeddings.shape[1]  # 384 (vektÃ¶r boyutu)
index = faiss.IndexFlatL2(dim)  # L2 mesafesi kullanÄ±r
index.add(embeddings)  # VektÃ¶rleri index'e ekler
```

**Ne Oluyor?**
- `IndexFlatL2`: DÃ¼z L2 (Ã–klid) mesafesi kullanan index tipi
- `index.add()`: TÃ¼m vektÃ¶rleri index'e ekler
- Index iÃ§inde vektÃ¶rler Ã¶zel formatta saklanÄ±r

**FAISS Index Ä°Ã§eriÄŸi:**
```
Index Tipi: IndexFlatL2
VektÃ¶r Boyutu: 384
VektÃ¶r SayÄ±sÄ±: 7

VektÃ¶r 0: [0.234, -0.567, ..., 0.123]
VektÃ¶r 1: [0.245, -0.578, ..., 0.134]
...
VektÃ¶r 6: [0.256, -0.589, ..., 0.145]
```

**7. ADIM: Dosyalara Kaydetme**

```python
faiss.write_index(index, self.index_path)  # index/faiss.index
```

**faiss.index dosyasÄ±:**
- Binary (ikili) format
- Ä°Ã§inde sadece sayÄ±lar (vektÃ¶rler)
- YaklaÅŸÄ±k boyut: vektÃ¶r_sayÄ±sÄ± Ã— 384 Ã— 4 byte

**metadata.json dosyasÄ±:**
- JSON format (okunabilir metin)
- Her chunk'Ä±n bilgileri
- Ä°nsan tarafÄ±ndan okunabilir

**doc_metadata.json dosyasÄ±:**
- JSON format
- Belge genel bilgileri
- Belge listesini gÃ¶stermek iÃ§in kullanÄ±lÄ±r

**Ã‡Ä±ktÄ± Ã–rneÄŸi:**
```
Index oluÅŸturuldu â†’ 7 chunk
Belge sayÄ±sÄ±: 2
```

**Zamanlama:**
- KÃ¼Ã§Ã¼k veri seti (100 chunk): ~2-3 saniye
- Orta veri seti (1000 chunk): ~10-15 saniye
- BÃ¼yÃ¼k veri seti (10000 chunk): ~2-3 dakika
- Model ilk yÃ¼klemede indirilir (yaklaÅŸÄ±k 90MB)

---

#### `load_index(self)`
**SatÄ±rlar:** 141-157

**Ne Yapar:**
- KaydedilmiÅŸ FAISS index'ini yÃ¼kler
- Metadata dosyalarÄ±nÄ± okur
- Index'i belleÄŸe alÄ±r (hÄ±zlÄ± arama iÃ§in)

**DÃ¶nÃ¼ÅŸ DeÄŸeri:**
- `True`: BaÅŸarÄ±lÄ± yÃ¼kleme
- `False`: Dosyalar bulunamadÄ±

**Kontroller:**
- ÃœÃ§ dosyanÄ±n da (`faiss.index`, `metadata.json`, `doc_metadata.json`) varlÄ±ÄŸÄ±nÄ± kontrol eder
- Eksik dosya varsa yÃ¼kleme yapmaz

---

#### `search(self, query, k=5)`
**SatÄ±rlar:** 160-193

**Ne Yapar:**
- KullanÄ±cÄ± sorgusunu vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
- FAISS ile en yakÄ±n k chunk'Ä± bulur
- SonuÃ§larÄ± benzerlik skoruna gÃ¶re sÄ±ralar

**Parametreler:**
- `query`: Arama sorgusu (string)
  - Ã–rnek: `"Makine Ã¶ÄŸrenmesi nedir?"`
- `k`: DÃ¶ndÃ¼rÃ¼lecek sonuÃ§ sayÄ±sÄ± (varsayÄ±lan: 5)
  - En yakÄ±n 5 chunk bulunur

**DetaylÄ± Ä°ÅŸlem AdÄ±mlarÄ±:**

**1. ADIM: Dosya KontrolÃ¼**

```python
if not os.path.exists(self.index_path) or not os.path.exists(self.metadata_path):
    return []  # Index yoksa boÅŸ liste dÃ¶ndÃ¼r
```

**Ne Kontrol Edilir?**
- `index/faiss.index` dosyasÄ± var mÄ±?
- `index/metadata.json` dosyasÄ± var mÄ±?
- Eksikse arama yapÄ±lamaz

**2. ADIM: Index YÃ¼kleme**

```python
if self.index is None:
    if not self.load_index():
        return []  # YÃ¼klenemezse boÅŸ liste
```

**Ne Oluyor?**
- Index bellekte yoksa (ilk kullanÄ±m veya yeniden baÅŸlatma)
- `load_index()` fonksiyonu Ã§aÄŸrÄ±lÄ±r
- FAISS index ve metadata'lar belleÄŸe alÄ±nÄ±r

**3. ADIM: Sorgu VektÃ¶rleÅŸtirme**

```python
q_vec = self.model.encode([query]).astype("float32")
```

**Ne Oluyor?**

KullanÄ±cÄ± sorgusu: `"Makine Ã¶ÄŸrenmesi nedir?"`

Bu sorgu aynÄ± SentenceTransformer modeli ile vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r:

```python
# Sorgu metni
query = "Makine Ã¶ÄŸrenmesi nedir?"

# VektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rme
q_vec = model.encode([query])

# SonuÃ§: 384 boyutlu vektÃ¶r
q_vec = [[0.234, -0.567, 0.891, ..., 0.123]]  # 1 Ã— 384 boyutlu array
```

**Neden Liste Ä°Ã§inde?**
- Model batch (toplu) iÅŸleme bekler
- `[query]` = 1 elemanlÄ± liste
- SonuÃ§ da 2D array: `[[...]]`

**4. ADIM: FAISS ile Arama**

```python
distances, indices = self.index.search(q_vec, min(k, self.index.ntotal))
```

**FAISS.search() Ne Yapar?**

FAISS, sorgu vektÃ¶rÃ¼nÃ¼ index'teki tÃ¼m vektÃ¶rlerle karÅŸÄ±laÅŸtÄ±rÄ±r:

```
Sorgu VektÃ¶rÃ¼:     [0.234, -0.567, ..., 0.123]

Index'teki VektÃ¶rler:
  VektÃ¶r 0:        [0.240, -0.560, ..., 0.125]  â†’ Mesafe: 0.85
  VektÃ¶r 1:        [0.100, -0.200, ..., 0.050]  â†’ Mesafe: 2.34
  VektÃ¶r 2:        [0.235, -0.568, ..., 0.124]  â†’ Mesafe: 0.12  â† EN YAKIN!
  VektÃ¶r 3:        [0.500, -0.800, ..., 0.300]  â†’ Mesafe: 4.56
  ...
```

**L2 Mesafesi (Ã–klid Mesafesi) NasÄ±l HesaplanÄ±r?**

```
Mesafe = âˆš[(0.234-0.235)Â² + (-0.567-(-0.568))Â² + ... + (0.123-0.124)Â²]
       = âˆš[0.000001 + 0.000001 + ... + 0.000001]
       = 0.012 (yaklaÅŸÄ±k)
```

**DÃ¶nen DeÄŸerler:**

```python
distances = [[0.12, 0.85, 1.23, 1.45, 1.67]]  # En yakÄ±n 5'in mesafeleri
indices = [[2, 0, 5, 8, 12]]                  # Hangi chunk'lar? (index numaralarÄ±)
```

**Mesafe AnlamÄ±:**
- **0.0 - 1.0:** Ã‡ok benzer (yÃ¼ksek benzerlik)
- **1.0 - 2.0:** Benzer (orta benzerlik)
- **2.0+:** FarklÄ± (dÃ¼ÅŸÃ¼k benzerlik)

**5. ADIM: Metadata'dan Metinleri Alma**

```python
for idx, dist in zip(indices[0], distances[0]):
    if idx < len(self.docs):
        results.append({
            "text": self.docs[idx]["text"],        # Chunk'Ä±n metnini al
            "score": float(dist),                  # Mesafe skoru
            "doc_name": self.docs[idx]["doc_name"], # Belge adÄ±
            "doc_id": self.docs[idx]["doc_id"]     # Belge ID
        })
```

**Ne Oluyor?**

Index 2 â†’ `metadata.json`'daki 2. elemana bak:
```json
{
    "doc_id": 0,
    "chunk_id": 1,
    "text": "Makine Ã¶ÄŸrenmesi (ML), bilgisayarlarÄ±n...",
    "doc_name": "makine_ogrenmesi.txt",
    "doc_hash": "..."
}
```

**SonuÃ§ FormatÄ±:**

```python
[
    {
        "text": "Makine Ã¶ÄŸrenmesi (ML), bilgisayarlarÄ±n aÃ§Ä±kÃ§a programlanmadan veriden Ã¶ÄŸrenmesini saÄŸlayan algoritmalar bÃ¼tÃ¼nÃ¼dÃ¼r...",
        "score": 0.12,  # Mesafe (dÃ¼ÅŸÃ¼k = iyi)
        "doc_name": "makine_ogrenmesi.txt",
        "doc_id": 0
    },
    {
        "text": "...",
        "score": 0.85,
        "doc_name": "makine_ogrenmesi.txt",
        "doc_id": 0
    },
    ...
]
```

**GÃ¶rsel Ã–zet:**

```
1. KullanÄ±cÄ±: "Makine Ã¶ÄŸrenmesi nedir?"
                    â†“
2. VektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼r: [0.234, -0.567, ..., 0.123]
                    â†“
3. FAISS'te ara: En yakÄ±n 5 vektÃ¶rÃ¼ bul
                    â†“
4. Mesafeler: [0.12, 0.85, 1.23, 1.45, 1.67]
   Indexler:  [2,    0,    5,    8,    12]
                    â†“
5. Metadata'dan metinleri al:
   Index 2 â†’ "Makine Ã¶ÄŸrenmesi (ML)..."
   Index 0 â†’ "..."
   ...
                    â†“
6. SonuÃ§larÄ± dÃ¶ndÃ¼r (skor sÄ±rasÄ±na gÃ¶re)
```

**Performans:**
- Arama sÃ¼resini Ã¶lÃ§er: `time.time() - start_time`
- Genellikle 10-50 milisaniye arasÄ±
- YazdÄ±rÄ±r: `"Arama 0.0234 saniyede tamamlandÄ±."`

**Ã–nemli Notlar:**

1. **Skor = Mesafe:**
   - DÃ¼ÅŸÃ¼k skor = YÃ¼ksek benzerlik (iyi sonuÃ§)
   - YÃ¼ksek skor = DÃ¼ÅŸÃ¼k benzerlik (kÃ¶tÃ¼ sonuÃ§)
   - Ã–rnek: 0.12 < 0.85 (ilk sonuÃ§ daha iyi)

2. **SonuÃ§lar Otomatik SÄ±ralÄ±:**
   - FAISS en yakÄ±ndan en uzaÄŸa sÄ±ralar
   - En iyi sonuÃ§ ilk sÄ±rada

3. **GÃ¼venlik KontrolÃ¼:**
   ```python
   if idx < len(self.docs):  # Index sÄ±nÄ±rlarÄ±nÄ± kontrol et
   ```
   - HatalÄ± index numarasÄ±nÄ± Ã¶nler

---

#### `get_document_list(self)`
**SatÄ±rlar:** 196-200

**Ne Yapar:**
- YÃ¼klenen belgelerin listesini dÃ¶ndÃ¼rÃ¼r
- Belge metadata'sÄ± yÃ¼klÃ¼ deÄŸilse Ã¶nce yÃ¼kler

**DÃ¶nÃ¼ÅŸ DeÄŸeri:**
- Belge bilgilerini iÃ§eren liste

**KullanÄ±m:**
- Streamlit arayÃ¼zÃ¼nde belge seÃ§imi iÃ§in kullanÄ±lÄ±r

---

#### `search_with_document_filter(self, query, doc_id=None, k=5)`
**SatÄ±rlar:** 202-270

**Ne Yapar:**
- Belirli bir belgede arama yapar (filtreli arama)
- Veya tÃ¼m belgelerde arama yapar (doc_id None ise)

**Parametreler:**
- `query`: Arama sorgusu
- `doc_id`: Belirli bir belgede arama yapmak iÃ§in (None = tÃ¼m belgeler)
- `k`: SonuÃ§ sayÄ±sÄ±

**Ä°ÅŸlem MantÄ±ÄŸÄ±:**

1. **Belge Filtreleme:**
   ```python
   filtered_indices = [i for i, doc in enumerate(self.docs) 
                       if doc.get('doc_id') == doc_id]
   ```
   - Sadece ilgili belgeye ait chunk'larÄ± bulur

2. **FiltrelenmiÅŸ VektÃ¶rler:**
   ```python
   filtered_embeddings = np.array([self.index.reconstruct(i) 
                                    for i in filtered_indices])
   ```
   - FAISS'ten sadece ilgili vektÃ¶rleri geri oluÅŸturur

3. **Manuel Mesafe Hesaplama:**
   ```python
   dist = np.linalg.norm(q_vec[0] - emb)
   ```
   - Her vektÃ¶r iÃ§in L2 mesafesi hesaplanÄ±r

4. **SÄ±ralama:**
   - Mesafelere gÃ¶re sÄ±ralanÄ±r
   - En yakÄ±n k sonuÃ§ seÃ§ilir

**KullanÄ±m:**
- Streamlit'te kullanÄ±cÄ± belirli bir belge seÃ§erse bu fonksiyon Ã§aÄŸrÄ±lÄ±r

---

#### `summarize(self, text, max_length=300, min_length=100)`
**SatÄ±rlar:** 273-346

**Ne Yapar:**
- Metni algoritmik olarak Ã¶zetler
- ML modeli kullanmaz, basit kurallara gÃ¶re Ã¶zet Ã§Ä±karÄ±r

**Parametreler:**
- `text`: Ã–zetlenecek metin
- `max_length`: Maksimum Ã¶zet uzunluÄŸu (kullanÄ±lmÄ±yor)
- `min_length`: Minimum Ã¶zet uzunluÄŸu (kullanÄ±lmÄ±yor)

**Ã–zetleme Stratejisi:**

1. **KÄ±sa Metinler:**
   - 200 karakterden kÄ±sa ise doÄŸrudan dÃ¶ndÃ¼rÃ¼r

2. **Paragraf BazlÄ±:**
   - Metni paragraflara ayÄ±rÄ±r
   - Ä°lk 2 ve son 2 paragrafÄ± alÄ±r
   - Her paragraftan ilk ve son cÃ¼mleyi seÃ§er

3. **CÃ¼mle BazlÄ±:**
   - Paragraf yoksa cÃ¼mle bazlÄ± Ã¶zetler
   - Ä°lk 3, ortadan 2, son 3 cÃ¼mleyi alÄ±r

4. **Kelime BazlÄ± (Yedek):**
   - Hata durumunda ilk 100 ve son 100 kelimeyi alÄ±r

**Not:**
- Bu basit bir Ã¶zetleme yÃ¶ntemidir
- ML tabanlÄ± Ã¶zetleme iÃ§in `transformers` kÃ¼tÃ¼phanesindeki Ã¶zetleme modelleri kullanÄ±labilir

---

#### `answer_question(self, context, question)`
**SatÄ±rlar:** 349-391

**Ne Yapar:**
- Verilen baÄŸlam (context) iÃ§inde soruya cevap verir
- BERT tabanlÄ± TÃ¼rkÃ§e soru-cevap modeli kullanÄ±r

**Parametreler:**
- `context`: Soruya cevap vermek iÃ§in kullanÄ±lacak metin
- `question`: Sorulan soru

**DÃ¶nÃ¼ÅŸ DeÄŸeri:**
- Tuple: `(cevap, gÃ¼ven_skoru)`
  - `cevap`: Bulunan cevap metni
  - `gÃ¼ven_skoru`: 0.0-1.0 arasÄ± skor (yÃ¼ksek = gÃ¼venilir)

**Ä°ÅŸlem AdÄ±mlarÄ±:**

1. **Girdi KontrolÃ¼:**
   - BoÅŸ context veya soru kontrolÃ¼ yapar

2. **BaÄŸlam KÄ±saltma:**
   - BERT modelleri maksimum 512 token kabul eder
   - 1024 karakterden uzunsa kÄ±saltÄ±r

3. **Model YÃ¼kleme (Lazy Loading):**
   ```python
   if self.qa_pipeline is None:
       self.qa_pipeline = pipeline("question-answering", 
                                   model="savasy/bert-base-turkish-squad")
   ```
   - Ä°lk Ã§aÄŸrÄ±da model yÃ¼klenir (yaklaÅŸÄ±k 500MB)
   - TÃ¼rkÃ§e model yÃ¼klenemezse Ä°ngilizce yedek model dener

4. **Soru-Cevap:**
   ```python
   result = self.qa_pipeline(question=question, context=context)
   ```
   - BERT modeli baÄŸlam iÃ§inde sorunun cevabÄ±nÄ± bulur
   - CevabÄ±n baÅŸlangÄ±Ã§ ve bitiÅŸ pozisyonlarÄ±nÄ± belirler

**KullanÄ±lan Model:**
- `savasy/bert-base-turkish-squad`: TÃ¼rkÃ§e iÃ§in eÄŸitilmiÅŸ BERT modeli
- SQuAD veri seti formatÄ±nda eÄŸitilmiÅŸtir

**Hata YÃ¶netimi:**
- Model yÃ¼klenemezse hata mesajÄ± dÃ¶ndÃ¼rÃ¼r
- Pipeline hatasÄ± durumunda exception yakalanÄ±r

---

## 2ï¸âƒ£ build_index.py - Index OluÅŸturma Scripti

**DosyanÄ±n AmacÄ±:** Komut satÄ±rÄ±ndan Ã§alÄ±ÅŸtÄ±rÄ±larak `data/` dizinindeki belgelerden index oluÅŸturur.

### ğŸ“ Fonksiyonlar

#### `load_documents_from_data_dir()`
**SatÄ±rlar:** 4-45

**Ne Yapar:**
- `data/` dizinindeki tÃ¼m TXT ve PDF dosyalarÄ±nÄ± okur
- Her dosyanÄ±n iÃ§eriÄŸini ve ismini dÃ¶ndÃ¼rÃ¼r

**Ä°ÅŸlem AdÄ±mlarÄ±:**

1. **Dizin KontrolÃ¼:**
   ```python
   if not os.path.exists("data"):
       os.makedirs("data")
   ```
   - `data/` dizini yoksa oluÅŸturur

2. **Dosya Okuma:**
   - `.txt` dosyalarÄ±: Direkt UTF-8 olarak okunur
   - `.pdf` dosyalarÄ±: `SearchEngine.load_pdf()` ile metin Ã§Ä±karÄ±lÄ±r

3. **Hata YÃ¶netimi:**
   - Desteklenmeyen dosya tipleri iÃ§in uyarÄ±
   - Okuma hatalarÄ±nda exception yakalama

**DÃ¶nÃ¼ÅŸ DeÄŸeri:**
- Tuple: `(documents, doc_names)`
  - `documents`: Metin iÃ§eriklerinin listesi
  - `doc_names`: Dosya isimlerinin listesi

---

#### `if __name__ == "__main__":`
**SatÄ±rlar:** 47-55

**Ne Yapar:**
- Script doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda:
  1. `data/` dizininden belgeleri yÃ¼kler
  2. SearchEngine oluÅŸturur
  3. Index'i oluÅŸturur

**KullanÄ±m:**
```bash
python build_index.py
```

**Ã‡Ä±ktÄ±:**
```
âœ… makine_ogrenmesi.txt dosyasÄ± yÃ¼klendi. (15234 karakter)
âœ… yapay_zeka.txt dosyasÄ± yÃ¼klendi. (8932 karakter)
Toplam 2 dÃ¶kÃ¼man yÃ¼klendi.
Index oluÅŸturuldu â†’ 125 chunk
Belge sayÄ±sÄ±: 2
Index baÅŸarÄ±yla oluÅŸturuldu!
```

---

## 3ï¸âƒ£ app.py - FastAPI REST API Sunucusu

**DosyanÄ±n AmacÄ±:** Web API sunucusu oluÅŸturur. Uzak uygulamalarÄ±n arama yapmasÄ±nÄ± saÄŸlar.

### ğŸ“ Kod AÃ§Ä±klamasÄ±

#### FastAPI UygulamasÄ±
**SatÄ±rlar:** 1-23

**Ne Yapar:**
- RESTful API endpoint'leri saÄŸlar
- CORS (Cross-Origin Resource Sharing) desteÄŸi ekler
- SearchEngine'i baÅŸlatÄ±r ve index yÃ¼kler

**Kurulum:**
```python
app = FastAPI()
engine = SearchEngine()
engine.load_index()
```

**CORS AyarlarÄ±:**
```python
app.add_middleware(CORSMiddleware,
                   allow_origins=["*"],      # TÃ¼m kaynaklardan izin
                   allow_methods=["*"],      # TÃ¼m HTTP metodlarÄ±
                   allow_headers=["*"])      # TÃ¼m header'lar
```
- TÃ¼m domain'lerden isteklere izin verir (geliÅŸtirme amaÃ§lÄ±)

---

#### Query Modeli
**SatÄ±rlar:** 17-18

**Ne Yapar:**
- API isteklerinin formatÄ±nÄ± tanÄ±mlar
- Pydantic ile veri doÄŸrulama yapar

**YapÄ±:**
```python
class Query(BaseModel):
    text: str  # Arama sorgusu
```

---

#### `/search` Endpoint
**SatÄ±rlar:** 20-23

**Ne Yapar:**
- POST isteÄŸi ile arama yapar
- JSON formatÄ±nda sorgu alÄ±r
- SonuÃ§larÄ± JSON formatÄ±nda dÃ¶ndÃ¼rÃ¼r

**Ä°stek Ã–rneÄŸi:**
```bash
curl -X POST "http://localhost:8000/search" \
     -H "Content-Type: application/json" \
     -d '{"text": "makine Ã¶ÄŸrenmesi nedir?"}'
```

**YanÄ±t Ã–rneÄŸi:**
```json
{
  "results": [
    {
      "text": "Makine Ã¶ÄŸrenmesi (ML), bilgisayarlarÄ±n...",
      "score": 0.85,
      "doc_name": "makine_ogrenmesi.txt",
      "doc_id": 0
    },
    ...
  ]
}
```

**KullanÄ±m:**
```bash
uvicorn app:app --reload
```
- VarsayÄ±lan port: 8000
- `--reload`: DeÄŸiÅŸikliklerde otomatik yeniden baÅŸlatma

---

## 4ï¸âƒ£ streamlit_app.py - Web ArayÃ¼zÃ¼

**DosyanÄ±n AmacÄ±:** KullanÄ±cÄ± dostu web arayÃ¼zÃ¼ saÄŸlar. KullanÄ±cÄ±lar tarayÄ±cÄ±dan belge yÃ¼kleyip arama yapabilir.

### ğŸ“ Fonksiyonlar

#### `check_memory_usage()`
**SatÄ±rlar:** 9-21

**Ne Yapar:**
- UygulamanÄ±n bellek kullanÄ±mÄ±nÄ± kontrol eder
- %80'den fazla kullanÄ±m varsa uyarÄ± verir ve bellek temizler

**KullanÄ±m:**
- Ã–zetleme iÅŸleminden Ã¶nce Ã§aÄŸrÄ±lÄ±r
- YÃ¼ksek bellek kullanÄ±mÄ±nÄ± Ã¶nlemek iÃ§in

---

#### `timeout(seconds)`
**SatÄ±rlar:** 23-30

**Ne Yapar:**
- Ä°ÅŸlemler iÃ§in zaman aÅŸÄ±mÄ± kontrolÃ¼ saÄŸlar
- Windows'ta tam timeout deÄŸil, sadece sÃ¼re Ã¶lÃ§Ã¼mÃ¼ yapar

**KullanÄ±m:**
- Soru-cevap iÅŸlemlerinde 30 saniye timeout

---

### ğŸ¨ Streamlit ArayÃ¼z BileÅŸenleri

#### Sayfa YapÄ±landÄ±rmasÄ±
**SatÄ±rlar:** 32-37

```python
st.set_page_config(
    page_title="Semantic Search Engine",
    page_icon="ğŸ”",
    layout="wide"
)
```

---

#### SearchEngine Cache
**SatÄ±rlar:** 49-54

**Ne Yapar:**
- SearchEngine nesnesini cache'ler
- Her sayfa yenilemesinde yeniden oluÅŸturmaz
- Performans iÃ§in Ã¶nemli

```python
@st.cache_resource
def get_search_engine():
    engine = SearchEngine()
    return engine
```

---

#### Index Durum KontrolÃ¼
**SatÄ±rlar:** 56-67

**Ne Yapar:**
- Index dosyalarÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol eder
- KullanÄ±cÄ±ya durum hakkÄ±nda bilgi verir
- BaÅŸarÄ±lÄ± yÃ¼kleme durumunda yeÅŸil uyarÄ± gÃ¶sterir

---

#### Sidebar Ayarlar
**SatÄ±rlar:** 69-71

- **SonuÃ§ SayÄ±sÄ± Slider:** 1-20 arasÄ± sonuÃ§ sayÄ±sÄ± seÃ§imi

---

#### Dosya YÃ¼kleme
**SatÄ±rlar:** 73-121

**Ne Yapar:**
- KullanÄ±cÄ±nÄ±n PDF/TXT dosyalarÄ±nÄ± yÃ¼klemesini saÄŸlar
- YÃ¼klenen dosyalarÄ± iÅŸler
- "Index OluÅŸtur" butonu ile index oluÅŸturur

**Ä°ÅŸlem AkÄ±ÅŸÄ±:**

1. **Dosya YÃ¼kleme:**
   ```python
   uploaded_files = st.sidebar.file_uploader(...)
   ```

2. **Ä°Ã§erik Ã‡Ä±karma:**
   - TXT: Direkt decode edilir
   - PDF: GeÃ§ici dosyaya kaydedilir, metin Ã§Ä±karÄ±lÄ±r, silinir

3. **Index OluÅŸturma:**
   ```python
   engine.build_index(documents, doc_names)
   ```
   - Cache temizlenir
   - Sayfa yeniden yÃ¼klenir

---

#### Belge SeÃ§imi
**SatÄ±rlar:** 123-135

**Ne Yapar:**
- KullanÄ±cÄ±nÄ±n belirli bir belgede arama yapmasÄ±nÄ± saÄŸlar
- Dropdown menÃ¼ ile belge seÃ§imi

**KullanÄ±m:**
- "TÃ¼m Belgeler" seÃ§ilirse tÃ¼m belgelerde arama
- Belirli belge seÃ§ilirse sadece o belgede arama

---

#### Ana Arama BÃ¶lÃ¼mÃ¼
**SatÄ±rlar:** 137-257

**Ä°ÅŸlem Tipleri:**

##### 1. Semantik Arama
**SatÄ±rlar:** 147-161

**Ne Yapar:**
- KullanÄ±cÄ± sorgusuna en benzer chunk'larÄ± bulur
- SonuÃ§larÄ± geniÅŸletilebilir (expandable) kutularda gÃ¶sterir

**GÃ¶rÃ¼ntÃ¼leme:**
- Her sonuÃ§ iÃ§in: Skor, belge adÄ±, chunk metni

---

##### 2. Soru Cevaplama
**SatÄ±rlar:** 163-200

**Ne Yapar:**
1. Ã–nce semantik arama ile en ilgili chunk'Ä± bulur (k=1)
2. Bu chunk'Ä± context olarak BERT modeline verir
3. Modelden cevabÄ± alÄ±r

**GÃ¶rÃ¼ntÃ¼leme:**
- Soru
- Cevap
- GÃ¼ven skoru (0-1 arasÄ±)
- Ä°ÅŸlem sÃ¼resi
- KullanÄ±lan context (geniÅŸletilebilir)

**Hata YÃ¶netimi:**
- Timeout kontrolÃ¼
- Exception yakalama ve gÃ¶sterim

---

##### 3. Ã–zet Ã‡Ä±kart
**SatÄ±rlar:** 202-252

**Ne Yapar:**
- TÃ¼m belgelerin Ã¶zetini Ã§Ä±karÄ±r
- Her belge ayrÄ± ayrÄ± Ã¶zetlenir

**Ä°ÅŸlem AkÄ±ÅŸÄ±:**

1. **Bellek KontrolÃ¼:**
   - YÃ¼ksek bellek kullanÄ±mÄ± varsa iÅŸlemi durdurur

2. **Belge Ä°ÅŸleme:**
   - Her belge iÃ§in:
     - 4000 karakterden uzunsa kÄ±saltÄ±r
     - `summarize()` fonksiyonunu Ã§aÄŸÄ±rÄ±r
     - Ã–zeti listeye ekler

3. **GÃ¶rÃ¼ntÃ¼leme:**
   - TÃ¼m Ã¶zetleri belge isimleriyle gÃ¶sterir
   - Ã–zetler arasÄ±nda Ã§izgi (divider) koyar

**Hata YÃ¶netimi:**
- Exception yakalama
- KullanÄ±cÄ±ya bilgilendirme mesajÄ±

---

#### Bilgi Kutusu
**SatÄ±rlar:** 259-265

- KullanÄ±cÄ±ya sistemin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± aÃ§Ä±klar
- Sidebar'da gÃ¶sterilir

---

## 5ï¸âƒ£ requirements.txt - Python BaÄŸÄ±mlÄ±lÄ±klarÄ±

**DosyanÄ±n AmacÄ±:** Projenin Ã§alÄ±ÅŸmasÄ± iÃ§in gerekli Python paketlerini listeler.

### ğŸ“¦ Paketler ve AÃ§Ä±klamalarÄ±

```
faiss-cpu              # FAISS'in CPU versiyonu (vektÃ¶r arama)
sentence-transformers  # Metin â†’ VektÃ¶r dÃ¶nÃ¼ÅŸÃ¼mÃ¼
numpy                  # SayÄ±sal iÅŸlemler (FAISS baÄŸÄ±mlÄ±lÄ±ÄŸÄ±)
PyPDF2                 # PDF dosyalarÄ±ndan metin Ã§Ä±karma
fastapi                # REST API framework
uvicorn                 # ASGI sunucu (FastAPI iÃ§in)
python-multipart       # Form verileri iÃ§in (FastAPI)
streamlit              # Web arayÃ¼zÃ¼ framework
transformers           # BERT ve diÄŸer ML modelleri
torch                  # PyTorch (transformers baÄŸÄ±mlÄ±lÄ±ÄŸÄ±)
psutil                 # Sistem kaynaklarÄ±nÄ± Ã¶lÃ§me (bellek)
```

**Kurulum:**
```bash
pip install -r requirements.txt
```

**Not:**
- `faiss-cpu`: GPU desteÄŸi iÃ§in `faiss-gpu` kullanÄ±labilir
- `torch`: Transformers'Ä±n Ã§alÄ±ÅŸmasÄ± iÃ§in gerekli

---

## ğŸ”„ Sistem AkÄ±ÅŸ DiyagramÄ±

### Index OluÅŸturma AkÄ±ÅŸÄ±

```
1. build_index.py Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r
   â†“
2. data/ dizinindeki dosyalar okunur
   â†“
3. Her belge chunk'lara bÃ¶lÃ¼nÃ¼r (200 kelime)
   â†“
4. Chunk'lar vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r (SentenceTransformer)
   â†“
5. FAISS index oluÅŸturulur ve kaydedilir
   â†“
6. Metadata JSON dosyalarÄ±na kaydedilir
```

### Arama AkÄ±ÅŸÄ±

```
1. KullanÄ±cÄ± sorgu girer
   â†“
2. Sorgu vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
   â†“
3. FAISS ile en yakÄ±n k vektÃ¶r bulunur (L2 mesafesi)
   â†“
4. SonuÃ§lar metadata'dan metin bilgisiyle birleÅŸtirilir
   â†“
5. Skorlara gÃ¶re sÄ±ralanÄ±r ve gÃ¶sterilir
```

### Soru-Cevap AkÄ±ÅŸÄ±

```
1. KullanÄ±cÄ± soru girer
   â†“
2. Semantik arama ile en ilgili chunk bulunur (k=1)
   â†“
3. Chunk context olarak BERT modeline verilir
   â†“
4. BERT modeli context iÃ§inde cevabÄ± bulur
   â†“
5. Cevap ve gÃ¼ven skoru gÃ¶sterilir
```

---

## ğŸ¯ KullanÄ±m SenaryolarÄ±

### Senaryo 1: Yeni Belge Ekleme

1. Belgeleri `data/` dizinine kopyala
2. `python build_index.py` Ã§alÄ±ÅŸtÄ±r
3. Index yeniden oluÅŸturulur

### Senaryo 2: Streamlit ile Arama

1. `streamlit run streamlit_app.py` Ã§alÄ±ÅŸtÄ±r
2. TarayÄ±cÄ±da aÃ§Ä±lan sayfada:
   - Dosya yÃ¼kle (opsiyonel)
   - Index oluÅŸtur (yeni dosya varsa)
   - Soru/arama ifadesi gir
   - Ä°ÅŸlem tipini seÃ§
   - "Ä°ÅŸlemi GerÃ§ekleÅŸtir" tÄ±kla

### Senaryo 3: API ile Arama

1. `uvicorn app:app --reload` Ã§alÄ±ÅŸtÄ±r
2. API'ye POST isteÄŸi gÃ¶nder:
   ```python
   import requests
   response = requests.post(
       "http://localhost:8000/search",
       json={"text": "makine Ã¶ÄŸrenmesi"}
   )
   results = response.json()["results"]
   ```

---

## ğŸ” Teknik Detaylar

### VektÃ¶r Boyutu
- **Model:** `sentence-transformers/all-MiniLM-L6-v2`
- **Boyut:** 384 boyutlu vektÃ¶rler
- **Dil DesteÄŸi:** Ã‡ok dilli (TÃ¼rkÃ§e dahil)

### FAISS Index Tipi
- **Tip:** `IndexFlatL2`
- **AÃ§Ä±klama:** DÃ¼z L2 (Ã–klid) mesafesi kullanÄ±r
- **Avantaj:** Kesin sonuÃ§ verir
- **Dezavantaj:** BÃ¼yÃ¼k veri setlerinde yavaÅŸ olabilir

**Alternatifler:**
- `IndexIVFFlat`: Daha hÄ±zlÄ±, yaklaÅŸÄ±k sonuÃ§lar
- `IndexHNSW`: HÄ±zlÄ± ve hassas (bÃ¼yÃ¼k veri setleri iÃ§in)

### Chunk Boyutu
- **VarsayÄ±lan:** 200 kelime
- **Neden:** 
  - Transformer modellerinin maksimum input uzunluÄŸu sÄ±nÄ±rlÄ±
  - Daha kÃ¼Ã§Ã¼k chunk'lar daha hassas arama saÄŸlar
  - Daha bÃ¼yÃ¼k chunk'lar daha fazla context iÃ§erir

---

## ğŸ› Bilinen SÄ±nÄ±rlamalar

1. **Ã–zetleme:**
   - ML modeli kullanmÄ±yor, basit algoritmik yÃ¶ntem
   - Ã‡ok uzun metinlerde kalite dÃ¼ÅŸebilir

2. **PDF Ä°ÅŸleme:**
   - Sadece metin Ã§Ä±karÄ±r, gÃ¶rselleri desteklemez
   - KarmaÅŸÄ±k layout'larda metin kaybolabilir

3. **Bellek:**
   - BÃ¼yÃ¼k veri setlerinde yÃ¼ksek bellek kullanÄ±mÄ±
   - FAISS index tamamen bellekte tutulur

4. **TÃ¼rkÃ§e Destek:**
   - Sentence Transformer Ã§ok dilli, TÃ¼rkÃ§e'yi destekler
   - QA modeli TÃ¼rkÃ§e eÄŸitilmiÅŸ (`savasy/bert-base-turkish-squad`)

---

## ğŸ“Š Performans Ä°puÃ§larÄ±

1. **Index Boyutu:**
   - 10.000 chunk'a kadar: IndexFlatL2 iyi Ã§alÄ±ÅŸÄ±r
   - Daha bÃ¼yÃ¼k veri setleri iÃ§in IndexIVFFlat veya IndexHNSW kullanÄ±n

2. **Chunk Boyutu:**
   - KÄ±sa chunk'lar (100-200 kelime): Daha hassas arama
   - Uzun chunk'lar (300-500 kelime): Daha fazla context

3. **K DeÄŸeri:**
   - Semantik arama iÃ§in: 5-10 yeterli
   - Soru-cevap iÃ§in: 1 yeterli (en ilgili chunk)

4. **Model SeÃ§imi:**
   - `all-MiniLM-L6-v2`: HÄ±zlÄ±, iyi kalite
   - `paraphrase-multilingual-MiniLM-L12-v2`: Daha iyi Ã§ok dilli destek

---

## ğŸš€ GeliÅŸtirme Ã–nerileri

1. **Hybrid Search:**
   - BM25 (keyword-based) + Semantic search birleÅŸtir
   - Hem anlamsal hem kelime eÅŸleÅŸmesi kullan

2. **Reranking:**
   - Cross-encoder model ile sonuÃ§larÄ± yeniden sÄ±rala
   - Daha hassas sonuÃ§lar iÃ§in

3. **Metadata Filtreleme:**
   - Tarih, kategori gibi metadata ile filtreleme
   - FAISS index'ine metadata ekle

4. **Ã–zetleme Ä°yileÅŸtirme:**
   - `facebook/bart-large-cnn` gibi Ã¶zetleme modelleri kullan
   - Daha kaliteli Ã¶zetler iÃ§in

---

## ğŸ“ SonuÃ§

Bu proje, modern bilgi eriÅŸim tekniklerini kullanarak:
- âœ… Semantik arama yapar
- âœ… Soru-cevap yeteneÄŸi sunar
- âœ… Belge Ã¶zetleme yapar
- âœ… KullanÄ±cÄ± dostu arayÃ¼z saÄŸlar
- âœ… REST API sunar

TÃ¼m kodlar TÃ¼rkÃ§e karakterleri destekler ve TÃ¼rkÃ§e belgeler Ã¼zerinde Ã§alÄ±ÅŸabilir.

---

---

## ğŸ“‹ Ã–ZET - Chunk ve FAISS Index KavramlarÄ±

### ğŸ”¸ Chunk (ParÃ§a) - KÄ±sa Ã–zet

**Ne?** BÃ¼yÃ¼k belgenin kÃ¼Ã§Ã¼k parÃ§alara bÃ¶lÃ¼nmÃ¼ÅŸ hali

**Neden?** 
- Model sÄ±nÄ±rlamalarÄ± (512 token)
- Hassas arama (sadece ilgili kÄ±sÄ±m)
- HÄ±zlÄ± iÅŸleme

**NasÄ±l?**
- Her 200 kelime bir chunk
- `chunk_text()` fonksiyonu ile bÃ¶lÃ¼nÃ¼r
- Her chunk ayrÄ± ayrÄ± vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r

**Ã–rnek:**
```
550 kelimelik belge â†’ 3 chunk (200, 200, 150 kelime)
```

**Ä°Ã§erik:**
```json
{
    "doc_id": 0,        // Hangi belge?
    "chunk_id": 0,      // KaÃ§Ä±ncÄ± parÃ§a?
    "text": "...",      // ParÃ§anÄ±n iÃ§eriÄŸi
    "doc_name": "..."
}
```

---

### ğŸ”¸ FAISS Index - KÄ±sa Ã–zet

**Ne?** VektÃ¶rlerin hÄ±zlÄ± arama iÃ§in saklandÄ±ÄŸÄ± binary dosya

**Ä°Ã§inde Ne Var?**
- Her chunk'Ä±n 384 boyutlu vektÃ¶r gÃ¶sterimi
- Index yapÄ±sÄ± (IndexFlatL2)
- VektÃ¶r organizasyonu

**Dosya:**
- `index/faiss.index` (binary, okunamaz)
- Boyut: ~188 KB (125 chunk iÃ§in)
- Okuma: Sadece `faiss.read_index()` ile

**NasÄ±l Ã‡alÄ±ÅŸÄ±r?**
```
1. Sorgu â†’ VektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼r
2. FAISS â†’ En yakÄ±n vektÃ¶rleri bul (mesafe hesaplama)
3. Index numaralarÄ±nÄ± al
4. Metadata'dan metinleri getir
5. SonuÃ§larÄ± gÃ¶ster
```

**Ã–rnek:**
```
Sorgu: "Makine Ã¶ÄŸrenmesi nedir?"
  â†“
VektÃ¶r: [0.234, -0.567, ..., 0.123]
  â†“
FAISS arama â†’ Index 2 (mesafe: 0.12)
  â†“
Metadata[2] â†’ "Makine Ã¶ÄŸrenmesi (ML)..."
  â†“
KullanÄ±cÄ±ya gÃ¶ster
```

---

### ğŸ”¸ ÃœÃ§ DosyanÄ±n Ä°ÅŸ BirliÄŸi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   faiss.index       â”‚  â†’ SayÄ±sal arama (hÄ±zlÄ±)
â”‚   (Binary)          â”‚     Index numaralarÄ±nÄ± verir
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   metadata.json     â”‚  â†’ Metin iÃ§eriÄŸi
â”‚   (Okunabilir)      â”‚     Index numarasÄ±na gÃ¶re metin verir
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ doc_metadata.json   â”‚  â†’ Belge bilgileri
â”‚   (Okunabilir)      â”‚     Belge listesi ve genel bilgiler
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**AkÄ±ÅŸ:**
1. FAISS â†’ En yakÄ±n 5 vektÃ¶r bul (index: 2, 0, 5, 8, 12)
2. Metadata â†’ Her index iÃ§in metni getir
3. Doc Metadata â†’ Belge adlarÄ±nÄ± getir
4. SonuÃ§larÄ± birleÅŸtir ve gÃ¶ster

---

### ğŸ”¸ Temel Terimler SÃ¶zlÃ¼ÄŸÃ¼

| Terim | AÃ§Ä±klama | Ã–rnek |
|-------|----------|-------|
| **Chunk** | Belgenin kÃ¼Ã§Ã¼k parÃ§asÄ± | 200 kelimelik metin parÃ§asÄ± |
| **Embedding** | Metnin sayÄ±sal gÃ¶sterimi | [0.234, -0.567, ..., 0.123] |
| **VektÃ¶r** | Embedding'in diÄŸer adÄ± | 384 boyutlu sayÄ± dizisi |
| **Index** | FAISS arama veritabanÄ± | faiss.index dosyasÄ± |
| **L2 Mesafesi** | Ä°ki vektÃ¶r arasÄ±ndaki uzaklÄ±k | 0.12 (dÃ¼ÅŸÃ¼k = benzer) |
| **Metadata** | Ek bilgiler (metin, belge adÄ± vb.) | JSON dosyalarÄ± |
| **Doc ID** | Belge numarasÄ± | 0, 1, 2... |
| **Chunk ID** | ParÃ§a numarasÄ± (belge iÃ§inde) | 0, 1, 2... |
| **Hash** | Belgenin benzersiz kodu | "eea0f046..." |

---

**Son GÃ¼ncelleme:** 2024
**Versiyon:** 2.0 (DetaylÄ± TÃ¼rkÃ§e AÃ§Ä±klamalÄ±)

