import faiss
import numpy as np
import json
import os
from sentence_transformers import SentenceTransformer
import logging
from transformers import pipeline
from datetime import datetime
import hashlib
import time

# Windows uyumluluÄŸu iÃ§in timeout
import sys
if sys.platform != "win32":
    from timeout_decorator import timeout, TimeoutError
else:
    # Windows iÃ§in mock timeout
    def timeout(seconds):
        def decorator(func):
            return func
        return decorator
    class TimeoutError(Exception):
        pass

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SearchEngine:
    def __init__(self, index_path="index/faiss.index", metadata_path="index/metadata.json", doc_metadata_path="index/doc_metadata.json"):
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.doc_metadata_path = doc_metadata_path
        self.model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        self.index = None
        self.docs = []
        self.doc_metadata = []
        
        # Dizinleri oluÅŸtur
        self._ensure_directories()
        
        # Ã–zetleme ve QA modellerini yÃ¼kle
        self.summarizer = None
        self.qa_pipeline = None

    def _ensure_directories(self):
        """Gerekli dizinleri oluÅŸturur"""
        index_dir = os.path.dirname(self.index_path)
        if index_dir and not os.path.exists(index_dir):
            os.makedirs(index_dir)
            logger.info(f"Dizin oluÅŸturuldu: {index_dir}")
    
    def _save_doc_metadata(self, doc_metadata):
        """Belge meta verisini kaydeder"""
        with open(self.doc_metadata_path, "w", encoding="utf-8") as f:
            json.dump(doc_metadata, f, ensure_ascii=False, indent=4)
        logger.info("Belge meta verisi kaydedildi.")
    
    def _load_doc_metadata(self):
        """Belge meta verisini yÃ¼kler"""
        if os.path.exists(self.doc_metadata_path):
            with open(self.doc_metadata_path, "r", encoding="utf-8") as f:
                self.doc_metadata = json.load(f)
            logger.info("Belge meta verisi yÃ¼klendi.")
            return True
        else:
            logger.warning("Belge meta verisi bulunamadÄ±.")
            return False

    # ================= CHUNKING ================= #
    def chunk_text(self, text, chunk_size=200):
        words = text.split()
        chunks = []
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)
        return chunks

    # ================= LOAD TEXT ================= #
    def load_pdf(self, path):
        from PyPDF2 import PdfReader
        reader = PdfReader(path)
        text = ""
        for page in reader.pages:
            try:
                text += page.extract_text()
            except Exception as e:
                logger.warning(f"Sayfa metni Ã§Ä±karÄ±lÄ±rken hata oluÅŸtu: {str(e)}")
                # BoÅŸ satÄ±r ekleyerek devam et
                text += "\n"
        return text

    # ================= INDEX OLUÅTURMA ================= #
    def build_index(self, documents, doc_names=None):
        all_chunks = []
        metadata = []
        doc_metadata = []
        
        # Belge isimleri saÄŸlanmamÄ±ÅŸsa varsayÄ±lan isimler oluÅŸtur
        if doc_names is None:
            doc_names = [f"Belge_{i+1}" for i in range(len(documents))]

        for doc_id, doc in enumerate(documents):
            # EÄŸer doc bir dict ise (PDF iÃ§in), path'ten oku
            if isinstance(doc, dict) and doc.get("type") == "pdf":
                doc = self.load_pdf(doc["path"])
            
            chunks = self.chunk_text(doc)
            doc_hash = hashlib.md5(doc.encode('utf-8')).hexdigest()
            doc_info = {
                "doc_id": doc_id,
                "name": doc_names[doc_id],
                "hash": doc_hash,
                "chunk_count": len(chunks),
                "created_at": datetime.now().isoformat()
            }
            doc_metadata.append(doc_info)
            
            for chunk_id, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                metadata.append({
                    "doc_id": doc_id,
                    "chunk_id": chunk_id,
                    "text": chunk,
                    "doc_name": doc_names[doc_id],
                    "doc_hash": doc_hash
                })

        embeddings = self.model.encode(all_chunks).astype("float32")

        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)
        index.add(embeddings)

        # Kaydet
        faiss.write_index(index, self.index_path)
        with open(self.metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        self._save_doc_metadata(doc_metadata)

        print(f"Index oluÅŸturuldu â†’ {len(all_chunks)} chunk")
        print(f"Belge sayÄ±sÄ±: {len(documents)}")

    # ================= Ä°NDEX YÃœKLE ================= #
    def load_index(self):
        if (os.path.exists(self.index_path) and 
            os.path.exists(self.metadata_path) and
            os.path.exists(self.doc_metadata_path)):
            self.index = faiss.read_index(self.index_path)
            with open(self.metadata_path, "r", encoding="utf-8") as f:
                self.docs = json.load(f)
            self._load_doc_metadata()
            print("ğŸ“¥ FAISS index yÃ¼klendi.")
            return True
        else:
            print("âš ï¸ Index dosyalarÄ± bulunamadÄ±.")
            # Ã–nceki verileri temizle
            self.index = None
            self.docs = []
            self.doc_metadata = []
            return False

    # ================= ARAMA ================= #
    def search(self, query, k=5):
        start_time = time.time()
        
        # Her seferinde index dosyalarÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et
        if not os.path.exists(self.index_path) or not os.path.exists(self.metadata_path):
            print("âš ï¸ Index dosyalarÄ± bulunamadÄ±.")
            return []
        
        # Index yÃ¼klÃ¼ deÄŸilse veya dosyalar deÄŸiÅŸmiÅŸse yeniden yÃ¼kle
        if self.index is None:
            if not self.load_index():
                return []

        # GÃ¼venlik kontrolÃ¼
        if self.index is None or not self.docs:
            print("âš ï¸ Index veya belgeler yÃ¼klenemedi.")
            return []

        q_vec = self.model.encode([query]).astype("float32")
        distances, indices = self.index.search(q_vec, min(k, self.index.ntotal))

        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.docs):  # Bounds check
                results.append({
                    "text": self.docs[idx]["text"],
                    "score": float(dist),
                    "doc_name": self.docs[idx].get("doc_name", "Bilinmiyor"),
                    "doc_id": self.docs[idx].get("doc_id", -1)
                })
        
        elapsed_time = time.time() - start_time
        print(f"Arama {elapsed_time:.4f} saniyede tamamlandÄ±.")
        return results
    
    # ================= BELGE Ä°ÅLEMLERÄ° ================= #
    def get_document_list(self):
        """YÃ¼klenen belgelerin listesini dÃ¶ndÃ¼rÃ¼r"""
        if not self.doc_metadata:
            self._load_doc_metadata()
        return self.doc_metadata
    
    def search_with_document_filter(self, query, doc_id=None, k=5):
        """Belirli bir belgede arama yapar"""
        # Index yÃ¼klÃ¼ deÄŸilse veya dosyalar deÄŸiÅŸmiÅŸse yeniden yÃ¼kle
        if not os.path.exists(self.index_path) or not os.path.exists(self.metadata_path):
            print("âš ï¸ Index dosyalarÄ± bulunamadÄ±.")
            return []
        
        if self.index is None:
            if not self.load_index():
                return []
        
        # GÃ¼venlik kontrolÃ¼
        if self.index is None or not self.docs:
            print("âš ï¸ Index veya belgeler yÃ¼klenemedi.")
            return []

        q_vec = self.model.encode([query]).astype("float32")
        
        # EÄŸer belirli bir belgede arama yapÄ±lacaksa
        if doc_id is not None:
            # Sadece ilgili belgeye ait chunk'larÄ± filtrele
            filtered_indices = [i for i, doc in enumerate(self.docs) if doc.get('doc_id') == doc_id]
            if not filtered_indices:
                return []
                
            # FiltrelenmiÅŸ indekslerde arama yap
            try:
                filtered_embeddings = np.array([self.index.reconstruct(i) for i in filtered_indices])
                distances = []
                indices = []
                
                for i, emb in enumerate(filtered_embeddings):
                    dist = np.linalg.norm(q_vec[0] - emb)
                    distances.append(dist)
                    indices.append(filtered_indices[i])
                
                # En yakÄ±n k sonuÃ§
                sorted_pairs = sorted(zip(distances, indices))[:min(k, len(distances))]
                
                results = []
                for dist, idx in sorted_pairs:
                    if idx < len(self.docs):  # Bounds check
                        results.append({
                            "text": self.docs[idx]["text"],
                            "score": float(dist),
                            "doc_name": self.docs[idx].get("doc_name", "Bilinmiyor"),
                            "doc_id": self.docs[idx].get("doc_id", -1)
                        })
                return results
            except Exception as e:
                print(f"Filtreli arama hatasÄ±: {e}")
                return []
        else:
            # TÃ¼m belgelerde arama
            try:
                distances, indices = self.index.search(q_vec, min(k, self.index.ntotal))
                results = []
                for idx, dist in zip(indices[0], distances[0]):
                    if idx < len(self.docs):  # Bounds check
                        results.append({
                            "text": self.docs[idx]["text"],
                            "score": float(dist),
                            "doc_name": self.docs[idx].get("doc_name", "Bilinmiyor"),
                            "doc_id": self.docs[idx].get("doc_id", -1)
                        })
                return results
            except Exception as e:
                print(f"Genel arama hatasÄ±: {e}")
                return []
    
    # ================= Ã–ZETLEME ================= #
    def summarize(self, text, max_length=300, min_length=100):
        """Metni Ã¶zetler - Daha ayrÄ±ntÄ±lÄ± sÃ¼rÃ¼m"""
        if not text or len(text.strip()) == 0:
            return "Ã–zetlenecek metin bulunamadÄ±."
        
        # Metin Ã§ok kÄ±sasa doÄŸrudan dÃ¶ndÃ¼r
        if len(text) < 200:
            return text
        
        # Daha ayrÄ±ntÄ±lÄ± Ã¶zetleme
        try:
            # Temel temizlik
            text = text.replace('\n', ' ').replace('\r', ' ').strip()
            
            # Paragraflara ayÄ±r
            paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 20]
            
            if len(paragraphs) >= 2:
                # Birden fazla paragraf varsa ilk ve son paragraflarÄ± al
                if len(paragraphs) <= 4:
                    # Az paragraf varsa hepsini kullan
                    summary_parts = paragraphs
                else:
                    # 5+ paragraf varsa ilk 2 ve son 2 paragrafÄ± al
                    summary_parts = paragraphs[:2] + ["..."] + paragraphs[-2:]
                
                # Her paragraftan Ã¶nemli cÃ¼mleleri seÃ§
                final_summary = []
                for part in summary_parts:
                    if part == "...":
                        final_summary.append(part)
                    else:
                        sentences = [s.strip() for s in part.split('.') if len(s.strip()) > 10]
                        if len(sentences) >= 2:
                            # Ä°lk ve son cÃ¼mleyi al
                            selected = [sentences[0]]
                            if len(sentences) > 2:
                                selected.append("...")
                            selected.append(sentences[-1])
                            final_summary.append(". ".join(selected) + ".")
                        else:
                            final_summary.append(part)
                
                return "\n\n".join(final_summary)
            
            # Paragraf yoksa cÃ¼mle bazlÄ± Ã¶zetle
            sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 15]
            
            if len(sentences) <= 6:
                # Az cÃ¼mle varsa hepsini dÃ¶ndÃ¼r
                return ". ".join(sentences) + "."
            else:
                # CÃ¼mle sayÄ±sÄ± fazlaysa daha fazlasÄ±nÄ± al
                mid_point = len(sentences) // 2
                selected_sentences = (
                    sentences[:3] +  # Ä°lk 3 cÃ¼mle
                    ["..."] +
                    sentences[mid_point-1:mid_point+1] +  # Ortadaki 2 cÃ¼mle
                    ["..."] +
                    sentences[-3:]  # Son 3 cÃ¼mle
                )
                return ". ".join(selected_sentences) + "."
                
        except Exception as e:
            logger.warning(f"Ã–zetleme hatasÄ±: {str(e)}")
            # Yedek yÃ¶ntem: Kelime bazlÄ± daha uzun Ã¶zet
            words = text.split()
            if len(words) <= 200:
                return text
            else:
                # Ä°lk 100 ve son 100 kelimeyi al
                start_words = words[:100]
                end_words = words[-100:] if len(words) > 200 else words[len(words)//2:]
                return " ".join(start_words) + "..." + " ".join(end_words)
    
    # ================= SORU CEVAP ================= #
    def answer_question(self, context, question):
        """Verilen baÄŸlamda soruya cevap verir"""
        if not context or not question:
            return "BaÄŸlam veya soru eksik.", 0.0
        
        if len(context.strip()) == 0 or len(question.strip()) == 0:
            return "BoÅŸ baÄŸlam veya soru.", 0.0
        
        # BaÄŸlam Ã§ok uzunsa kÄ±salt
        original_length = len(context)
        if len(context) > 1024:
            context = context[:1024]
            logger.info(f"BaÄŸlam kÄ±saltÄ±ldÄ±: {original_length} -> 1024 karakter")
        
        if self.qa_pipeline is None:
            # TÃ¼rkÃ§e destekli QA modeli
            try:
                logger.info("QA modeli yÃ¼kleniyor...")
                self.qa_pipeline = pipeline("question-answering", model="savasy/bert-base-turkish-squad")
                logger.info("QA modeli yÃ¼klendi")
            except Exception as e:
                logger.warning(f"TÃ¼rkÃ§e QA modeli yÃ¼klenemedi: {str(e)}")
                try:
                    # Yedek model
                    logger.info("Yedek QA modeli yÃ¼kleniyor...")
                    self.qa_pipeline = pipeline("question-answering")
                    logger.info("Yedek QA modeli yÃ¼klendi")
                except Exception as e2:
                    logger.error(f"Yedek QA modeli yÃ¼klenemedi: {str(e2)}")
                    return "QA modelleri yÃ¼klenemedi.", 0.0
        
        try:
            logger.info(f"Soru-cevap iÅŸlemi baÅŸlatÄ±lÄ±yor. BaÄŸlam: {len(context)} karakter, Soru: {len(question)} karakter")
            result = self.qa_pipeline(question=question, context=context)
            if result and 'answer' in result and 'score' in result:
                logger.info("Soru-cevap iÅŸlemi tamamlandÄ±")
                return result['answer'], result['score']
            else:
                logger.info("Cevap bulunamadÄ±")
                return "Cevap bulunamadÄ±.", 0.0
        except Exception as e:
            logger.error(f"Soru cevaplama hatasÄ±: {str(e)}")
            return f"Cevap bulunamadÄ±. Hata: {str(e)}", 0.0

